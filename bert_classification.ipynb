{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled2-2.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Fpw1-G3VjHMo","colab_type":"text"},"source":["**Introduction**\n","\n","Deep learning based natural language processing (NLP) systems have achieved state-of-the-art  results on various NLP tasks. Pre-trained language representations have been shown to improve many downstream NLP tasks such as question answering, and natural language inference. \n","\n","Fine tuning a pre-build BERT model approach is used for the given classification probelm.Due to high computation requirement of the approach:\n","\n","\n","\n","*   BERT pre-build model is loaded with an additional layer for classification.\n","*   Used a maximum sequence length of 256\n","\n","\n","*   Model is fine tuned on given dataset for 3 epochs\n","\n","**Libraries**\n","\n","\n","*   Bert prebuild model is loaded using tensorflow-hub\n","*   bert-tensorflow is used for transforming text to bert compactible format\n","\n","*   Keras is used for creating  model and fine tuning\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"qCZ0UEiryjSo","colab_type":"code","outputId":"38244b0b-4eff-4b26-f21c-7f9588eec8ff","executionInfo":{"status":"ok","timestamp":1565590971741,"user_tz":-330,"elapsed":6738,"user":{"displayName":"Abhilash A","photoUrl":"","userId":"00492578705542721639"}},"colab":{"base_uri":"https://localhost:8080/","height":207}},"source":["# import required modules\n","!pip install bert-tensorflow\n","import tensorflow as tf\n","import pandas as pd\n","import tensorflow_hub as hub\n","import os\n","import re\n","import numpy as np\n","from bert.tokenization import FullTokenizer\n","from tqdm import tqdm\n","from tensorflow.keras import backend as K\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n","\n","# Initialize session\n","sess = tf.Session()\n","\n","# Load datasets\n","train_df = pd.read_csv('train.csv')\n","test_df = pd.read_csv('test.csv')\n","\n","train_df['title_text'] = train_df['Review Text'] +' '+ train_df['Review Title']\n","test_df['title_text'] = test_df['Review Text'] +' '+ test_df['Review Title']\n","train_df = train_df.drop_duplicates(subset=['title_text'])\n","train_df.describe()"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: bert-tensorflow in /usr/local/lib/python3.6/dist-packages (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Review Text</th>\n","      <th>Review Title</th>\n","      <th>topic</th>\n","      <th>title_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>4217</td>\n","      <td>4217</td>\n","      <td>4217</td>\n","      <td>4217</td>\n","    </tr>\n","    <tr>\n","      <th>unique</th>\n","      <td>4196</td>\n","      <td>3727</td>\n","      <td>21</td>\n","      <td>4217</td>\n","    </tr>\n","    <tr>\n","      <th>top</th>\n","      <td>Doesn’t work</td>\n","      <td>Disappointed</td>\n","      <td>Bad Taste/Flavor</td>\n","      <td>That's the reason I gave this only 3 stars.  O...</td>\n","    </tr>\n","    <tr>\n","      <th>freq</th>\n","      <td>4</td>\n","      <td>14</td>\n","      <td>622</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         Review Text  ...                                         title_text\n","count           4217  ...                                               4217\n","unique          4196  ...                                               4217\n","top     Doesn’t work  ...  That's the reason I gave this only 3 stars.  O...\n","freq               4  ...                                                  1\n","\n","[4 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"S2fyClQ3rKjx","colab_type":"text"},"source":["**Preprocessing**\n","\n","Preprocessing data is kept simple and is directly adapted to format in which bert understands"]},{"cell_type":"code","metadata":{"id":"g5ZADuH8zGFP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"55389e50-53d3-436e-d34f-f56d233123a9","executionInfo":{"status":"ok","timestamp":1565590986507,"user_tz":-330,"elapsed":983,"user":{"displayName":"Abhilash A","photoUrl":"","userId":"00492578705542721639"}}},"source":["from keras.utils import to_categorical\n","max_seq_length = 256\n","le = LabelEncoder()\n","# Create datasets (Only take up to max_seq_length words for memory)\n","train_text = train_df[\"title_text\"].tolist()\n","train_text = [\" \".join(t.split()[0:max_seq_length]) for t in train_text]\n","train_text = np.array(train_text, dtype=object)[:, np.newaxis]\n","train_df['polarity'] = le.fit_transform(train_df['topic'])\n","train_label = to_categorical(train_df['polarity'].values).tolist()\n","# train_label = train_df[\"polarity\"].tolist()\n","\n","test_text = test_df[\"title_text\"].tolist()\n","test_text = [\" \".join(t.split()[0:max_seq_length]) for t in test_text]\n","test_text = np.array(test_text, dtype=object)[:, np.newaxis]"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"6lNWcZofzZgK","colab_type":"code","colab":{}},"source":["class PaddingInputExample(object):\n","    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n","  When running eval/predict on the TPU, we need to pad the number of examples\n","  to be a multiple of the batch size, because the TPU requires a fixed batch\n","  size. The alternative is to drop the last batch, which is bad because it means\n","  the entire output data won't be generated.\n","  We use this class instead of `None` because treating `None` as padding\n","  battches could cause silent errors.\n","  \"\"\"\n","\n","\n","class InputExample(object):\n","    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n","\n","    def __init__(self, guid, text_a, text_b=None, label=None):\n","        \"\"\"Constructs a InputExample.\n","    Args:\n","      guid: Unique id for the example.\n","      text_a: string. The untokenized text of the first sequence. For single\n","        sequence tasks, only this sequence must be specified.\n","      text_b: (Optional) string. The untokenized text of the second sequence.\n","        Only must be specified for sequence pair tasks.\n","      label: (Optional) string. The label of the example. This should be\n","        specified for train and dev examples, but not for test examples.\n","    \"\"\"\n","        self.guid = guid\n","        self.text_a = text_a\n","        self.text_b = text_b\n","        self.label = label\n","\n","\n","def create_tokenizer_from_hub_module(bert_path):\n","    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n","    bert_module = hub.Module(bert_path)\n","    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n","    vocab_file, do_lower_case = sess.run(\n","        [tokenization_info[\"vocab_file\"], tokenization_info[\"do_lower_case\"]]\n","    )\n","\n","    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n","\n","\n","def convert_single_example(tokenizer, example, max_seq_length=256):\n","    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n","\n","    if isinstance(example, PaddingInputExample):\n","        input_ids = [0] * max_seq_length\n","        input_mask = [0] * max_seq_length\n","        segment_ids = [0] * max_seq_length\n","        label = [0] * 21\n","        return input_ids, input_mask, segment_ids, label\n","\n","    tokens_a = tokenizer.tokenize(example.text_a)\n","    if len(tokens_a) > max_seq_length - 2:\n","        tokens_a = tokens_a[0: (max_seq_length - 2)]\n","\n","    tokens = []\n","    segment_ids = []\n","    tokens.append(\"[CLS]\")\n","    segment_ids.append(0)\n","    for token in tokens_a:\n","        tokens.append(token)\n","        segment_ids.append(0)\n","    tokens.append(\"[SEP]\")\n","    segment_ids.append(0)\n","\n","    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n","    # tokens are attended to.\n","    input_mask = [1] * len(input_ids)\n","\n","    # Zero-pad up to the sequence length.\n","    while len(input_ids) < max_seq_length:\n","        input_ids.append(0)\n","        input_mask.append(0)\n","        segment_ids.append(0)\n","\n","    assert len(input_ids) == max_seq_length\n","    assert len(input_mask) == max_seq_length\n","    assert len(segment_ids) == max_seq_length\n","    return input_ids, input_mask, segment_ids, example.label\n","\n","\n","def convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n","    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n","\n","    input_ids, input_masks, segment_ids, labels = [], [], [], []\n","    for example in tqdm(examples, desc=\"Converting examples to features\"):\n","        input_id, input_mask, segment_id, label = convert_single_example(\n","            tokenizer, example, max_seq_length\n","        )\n","        input_ids.append(input_id)\n","        input_masks.append(input_mask)\n","        segment_ids.append(segment_id)\n","        labels.append(label)\n","    return (\n","        np.array(input_ids),\n","        np.array(input_masks),\n","        np.array(segment_ids),\n","        np.array(labels),\n","    )\n","\n","\n","def convert_text_to_examples(texts, labels=None):\n","    \"\"\"Create InputExamples\"\"\"\n","    InputExamples = []\n","    if not labels:\n","        for text in texts:\n","            InputExamples.append(\n","                InputExample(guid=None, text_a=\" \".join(text), text_b=None, label=0)\n","            )\n","    else:\n","        for text, label in zip(texts, labels):\n","            InputExamples.append(\n","                InputExample(guid=None, text_a=\" \".join(text), text_b=None, label=label)\n","            )\n","    return InputExamples"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hKqOpIC5zdjs","colab_type":"code","outputId":"92f8b6eb-bc0a-465a-da8a-e61faa0b8e5b","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1565591010578,"user_tz":-330,"elapsed":14202,"user":{"displayName":"Abhilash A","photoUrl":"","userId":"00492578705542721639"}}},"source":["# Params for bert model and tokenization\n","bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n","# Instantiate tokenizer\n","tokenizer = create_tokenizer_from_hub_module(bert_path)\n","# Convert data to InputExample format\n","train_examples = convert_text_to_examples(train_text, train_label)\n","test_examples = convert_text_to_examples(test_text)\n","# Convert to features\n","(\n","    train_input_ids,\n","    train_input_masks,\n","    train_segment_ids,\n","    train_labels,\n",") = convert_examples_to_features(\n","    tokenizer, train_examples, max_seq_length=max_seq_length\n",")\n","(\n","    test_input_ids,\n","    test_input_masks,\n","    test_segment_ids,\n","    test_labels\n",") = convert_examples_to_features(\n","    tokenizer, test_examples, max_seq_length=max_seq_length\n",")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0812 06:23:20.061137 140650167068544 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n","\n","Converting examples to features: 100%|██████████| 4217/4217 [00:05<00:00, 753.49it/s]\n","Converting examples to features: 100%|██████████| 2553/2553 [00:03<00:00, 690.15it/s]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"KSHLSLkVr4Pp","colab_type":"text"},"source":["**Model Building**\n","\n","Basic bert uncased model is loaded using tensorflow-hub. A multiclass classification layer is attached to this model .Entire thing is packed in a keras model\n","\n","\n","*   max_seq_length = 256\n","\n","*   finetune_cell = 1\n","*   loss = categorical_crossentropy \n","\n","*   metrics=accuracy\n","\n","*   optimizer = adam\n","\n"]},{"cell_type":"code","metadata":{"id":"2v2VggQTzldz","colab_type":"code","colab":{}},"source":["class BERT(tf.keras.layers.Layer):\n","    def __init__(self, finetune_cells, debug=False, **kwargs):\n","        self.finetune_cells = finetune_cells\n","        self.trainable = True\n","        self.output_size = 768\n","        self.bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n","        self.debug = debug\n","        super(BERT, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        self.bert = hub.Module(self.bert_path,\n","                               trainable=self.trainable,\n","                               name=\"{}_module\".format(self.name))\n","\n","        trainable_vars = self.bert.variables\n","\n","        t_vs = [var for var in trainable_vars if not \"/cls/\" in var.name]\n","\n","        trainable_vars = t_vs\n","\n","        layer_name_list = []\n","\n","        for i, var in enumerate(trainable_vars):\n","            if self.debug:\n","                var_shape = var.get_shape()\n","                var_params = 1\n","                for dim in var_shape:\n","                    var_params *= dim\n","                print(str(i), \"-\", \"var:\", var.name)\n","                print(\" \", \"shape:\", var_shape, \"param:\", var_params)\n","\n","            if \"layer\" in var.name:\n","                layer_name = var.name.split(\"/\")[3]\n","                layer_name_list.append(layer_name)\n","\n","        layer_names = list(set(layer_name_list))\n","        layer_names.sort()\n","\n","        if self.debug:\n","            print(layer_names)\n","\n","        if self.finetune_cells == -1:\n","            for var in trainable_vars:\n","                if \"/pooler/\" in var.name:\n","                    # ignore the undocumented pooling layer\n","                    # we will create our own\n","                    pass\n","                else:\n","                    self._trainable_weights.append(var)\n","\n","        else:\n","            # Select how many layers to fine tune\n","            last_n_layers = len(layer_names) - self.finetune_cells\n","\n","            for var in trainable_vars:\n","                if \"layer\" in var.name:\n","                    layer_name = var.name.split(\"/\")[3]\n","                    layer_num = int(layer_name.split(\"_\")[1]) + 1\n","                    if layer_num > last_n_layers:\n","                        # Add to trainable weights\n","                        self._trainable_weights.append(var)\n","\n","            if self.debug:\n","                print(\"BERT module loaded with\", len(layer_names),\n","                      \"Transformer cells, training all cells >\", last_n_layers)\n","\n","        # Add non-trainable weights\n","        for var in self.bert.variables:\n","            if var not in self._trainable_weights:\n","                self._non_trainable_weights.append(var)\n","\n","        super(BERT, self).build(input_shape)\n","\n","    def call(self, inputs):\n","        input_ids, input_mask, segment_ids = inputs\n","        bert_inputs = dict(input_ids=tf.cast(input_ids, dtype=\"int32\"),\n","                           input_mask=tf.cast(input_mask, dtype=\"int32\"),\n","                           segment_ids=tf.cast(segment_ids, dtype=\"int32\"))\n","        result = self.bert(inputs=bert_inputs,\n","                           signature=\"tokens\",\n","                           as_dict=True)[\"sequence_output\"]\n","        return result\n","\n","    def compute_output_shape(self, input_shape):\n","        return (input_shape[0], self.output_size)\n","\n","# Build model\n","def build_model(max_seq_length):\n","    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n","    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n","    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n","    bert_inputs = [in_id, in_mask, in_segment]\n","\n","    bert_output = BERT(finetune_cells=1,\n","                       debug=False)(bert_inputs)\n","    dense = tf.keras.layers.GlobalMaxPooling1D()(bert_output)\n","    pred = tf.keras.layers.Dense(21, activation=\"softmax\")(dense)\n","\n","    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n","    model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n","    model.summary()\n","\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"i55HKRsBz5ao","colab_type":"code","outputId":"44e0e67e-7283-40c7-c182-b8012ddbcc57","colab":{"base_uri":"https://localhost:8080/","height":462},"executionInfo":{"status":"ok","timestamp":1565591035229,"user_tz":-330,"elapsed":13686,"user":{"displayName":"Abhilash A","photoUrl":"","userId":"00492578705542721639"}}},"source":[" model = build_model(max_seq_length)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["W0812 06:23:54.585210 140650167068544 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n"],"name":"stderr"},{"output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_ids (InputLayer)          [(None, 256)]        0                                            \n","__________________________________________________________________________________________________\n","input_masks (InputLayer)        [(None, 256)]        0                                            \n","__________________________________________________________________________________________________\n","segment_ids (InputLayer)        [(None, 256)]        0                                            \n","__________________________________________________________________________________________________\n","bert (BERT)                     (None, None, 768)    110104890   input_ids[0][0]                  \n","                                                                 input_masks[0][0]                \n","                                                                 segment_ids[0][0]                \n","__________________________________________________________________________________________________\n","global_max_pooling1d (GlobalMax (None, 768)          0           bert[0][0]                       \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 21)           16149       global_max_pooling1d[0][0]       \n","==================================================================================================\n","Total params: 110,121,039\n","Trainable params: 7,104,021\n","Non-trainable params: 103,017,018\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jCbjhBoAs918","colab_type":"text"},"source":["**Fine Tuning**\n","\n","Model is fine tuned on the given data:\n","\n","\n","*   epochs = 3\n","*   batch_size = 32\n","\n"]},{"cell_type":"code","metadata":{"id":"iF1UkmHN0FiS","colab_type":"code","outputId":"83c46e70-9fd4-4cd7-9776-12da51180dc5","colab":{"base_uri":"https://localhost:8080/","height":207},"executionInfo":{"status":"ok","timestamp":1565605824542,"user_tz":-330,"elapsed":14781709,"user":{"displayName":"Abhilash A","photoUrl":"","userId":"00492578705542721639"}}},"source":["  "],"execution_count":7,"outputs":[{"output_type":"stream","text":["W0812 06:24:07.216229 140650167068544 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/3\n","4217/4217 [==============================] - 4928s 1s/sample - loss: 1.4402 - acc: 0.5966\n","Epoch 2/3\n","4217/4217 [==============================] - 4922s 1s/sample - loss: 0.6705 - acc: 0.8029\n","Epoch 3/3\n","4217/4217 [==============================] - 4924s 1s/sample - loss: 0.3858 - acc: 0.8824\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7feb59786668>"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"xDgkuBxStkzD","colab_type":"text"},"source":["**Prediction**"]},{"cell_type":"code","metadata":{"id":"MPH4_119IhC5","colab_type":"code","outputId":"8fdcd2d7-1a7c-49ad-f341-008d0f8fd791","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1565609231062,"user_tz":-330,"elapsed":219509,"user":{"displayName":"Abhilash A","photoUrl":"","userId":"00492578705542721639"}}},"source":["prediction = model.predict([test_input_ids, test_input_masks, test_segment_ids])\n","print(prediction)\n","indexes = tf.argmax(prediction, axis=1)\n","# print(indexes.eval(session=sess))\n","# acc = accuracy_score(test_df['encoded_topic'].values,indexes.eval(session=sess))\n","# print(acc)\n","test_df['topic'] = le.inverse_transform(indexes.eval(session=sess))\n","print(test_df)\n","test_df.to_csv(\"output.csv\",index=False)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["[[1.05396949e-03 5.19321784e-02 8.30797537e-04 ... 8.79970496e-04\n","  1.01592101e-04 3.68278881e-04]\n"," [1.05396949e-03 5.19321784e-02 8.30797537e-04 ... 8.79970496e-04\n","  1.01592101e-04 3.68278881e-04]\n"," [2.99880821e-06 3.90453279e-05 5.04614727e-05 ... 1.98011116e-06\n","  1.20458273e-04 2.70103487e-06]\n"," ...\n"," [1.07484616e-01 8.12511444e-01 3.45771797e-02 ... 1.63244968e-03\n","  6.08290546e-04 1.08435315e-05]\n"," [2.51519406e-04 2.43572955e-04 9.55758969e-06 ... 2.44856230e-04\n","  8.65313996e-05 2.94867816e-04]\n"," [2.05402332e-03 9.88022313e-02 3.21825035e-02 ... 3.92770336e-04\n","  1.69116620e-03 2.30212845e-06]]\n","                                            Review Text  ...                  topic\n","0     I use chia seed in my protein shakes. These ta...  ...   Quality/Contaminated\n","1     I use chia seed in my protein shakes. These ta...  ...   Quality/Contaminated\n","2                               Don’t waste your money.  ...          Not Effective\n","3     I use the book 'Fortify Your Life' by Tieraona...  ...            Ingredients\n","4     I use the book 'Fortify Your Life' by Tieraona...  ...            Ingredients\n","5     I used to be loyal customer to this brand. I h...  ...   Quality/Contaminated\n","6     I used to be loyal customer to this brand. I h...  ...   Quality/Contaminated\n","7     I used to be loyal customer to this brand. I h...  ...   Quality/Contaminated\n","8     I used to be loyal customer to this brand. I h...  ...   Quality/Contaminated\n","9                            I haven’t received it yet.  ...  Shipment and delivery\n","10    I bought these for my girlfriend, who I couldn...  ...          Not Effective\n","11    The almonds were sealed, but not dusted separa...  ...              Packaging\n","12    I really wanted to give these a try and notice...  ...   Quality/Contaminated\n","13    I really wanted to give these a try and notice...  ...   Quality/Contaminated\n","14    I ordered Cherry Vanilla,  got BlackBerry. Cus...  ...       Bad Taste/Flavor\n","15    I ordered Cherry Vanilla,  got BlackBerry. Cus...  ...       Bad Taste/Flavor\n","16    Buyer beware: these vitamins are not technical...  ...               Allergic\n","17    Buyer beware: these vitamins are not technical...  ...               Allergic\n","18    My children weren't a fan of the flavors. They...  ...       Bad Taste/Flavor\n","19    Horrible, gritty, disgusting.  I have tried it...  ...       Bad Taste/Flavor\n","20    maybe good for you but I suggest they rework t...  ...       Bad Taste/Flavor\n","21    maybe good for you but I suggest they rework t...  ...       Bad Taste/Flavor\n","22    Not absorbent, not long.  Poise 6 is much,much...  ...          Not Effective\n","23    Very disappointed!  I made a mistake and order...  ...   Quality/Contaminated\n","24    Very disappointed!  I made a mistake and order...  ...   Quality/Contaminated\n","25    Very disappointed!  I made a mistake and order...  ...   Quality/Contaminated\n","26    Was broken Upon delivery. The top broke off in...  ...              Packaging\n","27    This product is essentially worthless. Does no...  ...          Not Effective\n","28    This product is essentially worthless. Does no...  ...          Not Effective\n","29    Note that I currently take Nature's Way Alive,...  ...   Quality/Contaminated\n","...                                                 ...  ...                    ...\n","2523  Some reviews said it smelled bad, and I think ...  ...   Quality/Contaminated\n","2524  Some reviews said it smelled bad, and I think ...  ...   Quality/Contaminated\n","2525  I bought this August 2018. I barely used it wh...  ...                 Expiry\n","2526  Bought these for my daughter thinking it would...  ...          Not Effective\n","2527  Bought these for my daughter thinking it would...  ...          Not Effective\n","2528  I bought these because I'm anemic late in my p...  ...            Ingredients\n","2529  Would give it a zero if I could. Never even re...  ...              Packaging\n","2530  Would give it a zero if I could. Never even re...  ...              Packaging\n","2531  I’ve purchased Kirkland Olive Oil in the past:...  ...       Bad Taste/Flavor\n","2532  The wild cherry flavor is enjoyable, albeit is...  ...              Too Sweet\n","2533  The wild cherry flavor is enjoyable, albeit is...  ...              Too Sweet\n","2534  The wild cherry flavor is enjoyable, albeit is...  ...              Too Sweet\n","2535  The wild cherry flavor is enjoyable, albeit is...  ...              Too Sweet\n","2536  After 2nd use, I woke in middle of the night w...  ...   Quality/Contaminated\n","2537  After 2nd use, I woke in middle of the night w...  ...   Quality/Contaminated\n","2538                                 Big time overrated  ...          Not Effective\n","2539  This collagen is unflavored so the taste is pa...  ...                Texture\n","2540  I wish I had read the reviews for the product ...  ...   Quality/Contaminated\n","2541  I wish I had read the reviews for the product ...  ...   Quality/Contaminated\n","2542                                            Rubbish  ...          Not Effective\n","2543  I tried half a scoop mixed in water and within...  ...               Allergic\n","2544  This does not stir in easily or dissolve. It’s...  ...       Bad Taste/Flavor\n","2545  All the bags in one of the rolls was miss cut ...  ...              Packaging\n","2546  Product burst opened in the box and created a ...  ...  Shipment and delivery\n","2547  This is the worse protein I've ever tried! The...  ...              Too Sweet\n","2548  This is the worse protein I've ever tried! The...  ...              Too Sweet\n","2549  Very small and easy to swallow- no flavor at a...  ...       Bad Taste/Flavor\n","2550  Very small and easy to swallow- no flavor at a...  ...       Bad Taste/Flavor\n","2551  Good but it increases the bad cholesterol- LDL...  ...   Quality/Contaminated\n","2552                     Will not buy the powder again.  ...       Customer Service\n","\n","[2553 rows x 4 columns]\n"],"name":"stdout"}]}]}
